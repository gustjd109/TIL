# WEEK16 DAY6 TIL

## 1. 오늘의 학습 목록
```
1. 음성 및 화면 공유 메인 프로젝트 연동
```

## 2. 16주차 6일차를 마치며...
우선적으로 백엔드에서 WebRTC 통신하기 위한 offer, answer, icecandidate를 주고받을 수 있도록 코드를 추가했다.  
이제 프론트엔드 쪽에서 서로 offer, aswer, icecandidate를 주고받고, 통신할 수 있는 상태가 되었을 때 서버에 접속하여 실시간 음성 및 화면 공유가 가능한지 테스트 과정이 필요하다.

개발을 진행하면서 한 가지 걱정되는 것이 몇 가지 있다.🧐

첫 번째, 현재 구현하고 있는 WebRTC가 P2P 방식인데, 멀티로 3명에서 5명 정도가 아닌 그 이상의 인원들이 서로 음성 및 화면 공유를 할 수 있는가이다.  
그래서 찾아본 결과, 현재 우리가 구현하고 있는 WebRTC는 가장 기본적인 Mesh 방식으로 구현하고 있는 것으로 판단된다.  
Mesh 방식을 사용했을 경우 1:1 통신에는 문제가 없지만, 5명 정도 이상만 되어도 클라이언트 측에 부하가 많이 발생하는 것으로 보인다.  
따라서 보통 실시간 통신을 하기 위해 SFU 방식을 많이 구현한다고 하며, ZOOM과 Google Meet에서 사용하는 방식으로 1,000명이 한 방에서 통신이 가능한 것으로 보인다.  
좀 더 확실하게 정보를 찾아 우리에게 맞는 방식으로 다시 바꿔야 할 수 있다는 것이 현재 걱정되는 부분이다.

두 번째, 우리 프로젝트에서는 채팅방에서 여러 사람이 스터디를 진행하는 플랫폼이다.  
채팅방에서는 텍스트, 음성(음성을 STT를 이용하여 텍스트로 변환), 이미지 텍스트(OCR로 이미지의 모든 텍스트를 추출) 모두 DB에 저장하고 사용한다.  
이때 우리 프로젝트의 핵심 기능인 스터디 전체 내용을 GPT를 이용하여 요약할 때, 어떤 기준으로 어떻게 텍스트, 음성, 이미지 텍스트가 서로 뒤바뀌지 않고 순서대로 정렬하여 요약할지에 대한 방법을 찾아야 한다는 것이다.  
사실 텍스트와 음성에서 추출한 텍스트만 이용했을 때는 간단하게 해결할 수 있을 것 같다.  
이 부분은 팀원들과 상의하면서 해결 방법을 찾아 봐야 할 것 같다.